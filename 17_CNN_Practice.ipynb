{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamanZargarzadeh/Deep-Learning/blob/main/17_CNN_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rffwkDWTz_Fo"
      },
      "source": [
        "# Image classifier for the SVHN dataset\n",
        "## Instructions\n",
        "\n",
        "In this notebook, you will create a neural network that classifies real-world images digits. You will use concepts from throughout this course in building, training, testing, validating and saving your Tensorflow classifier model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3vI8jSIz_Fs"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from scipy.io import loadmat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1hiCUc0Pel4"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Dropout, Conv2D, MaxPooling2D \n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.preprocessing import image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjGxC7YmPel4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OrHY7TRz_Fx"
      },
      "source": [
        "For this assignment, you will use the [SVHN dataset](http://ufldl.stanford.edu/housenumbers/). This is an image dataset of over 600,000 digit images in all, and is a harder dataset than MNIST as the numbers appear in the context of natural scene images. SVHN is obtained from house numbers in Google Street View images.\n",
        "\n",
        "* Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu and A. Y. Ng. \"Reading Digits in Natural Images with Unsupervised Feature Learning\". NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011.\n",
        "\n",
        "Your goal is to develop an end-to-end workflow for building, training, validating, evaluating and saving a neural network that classifies a real-world image into one of ten classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8BHW8P_2wxw"
      },
      "outputs": [],
      "source": [
        "# Run this cell to connect to your Drive folder\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q1n_Ai2z_F3"
      },
      "source": [
        "## 1. Load and preprocess the dataset\n",
        "* Extract the training and testing images and labels separately from the train and test dictionaries loaded for you.\n",
        "* Select a random sample of images and corresponding labels from the dataset (at least 10), and display them in a figure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qegFDfmjPel5"
      },
      "outputs": [],
      "source": [
        "# Load the dataset from your Drive folder\n",
        "\n",
        "train = loadmat('path/to/train_32x32.mat')\n",
        "test = loadmat('path/to/test_32x32.mat')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dbNe5psPel6"
      },
      "source": [
        "Both train and test are dictionaries with keys X and y for the input images and labels respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WIH5hyXz_F4"
      },
      "outputs": [],
      "source": [
        "X_train = None\n",
        "y_train = None\n",
        "X_test = None\n",
        "y_test = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmGJK3xgz_F8"
      },
      "outputs": [],
      "source": [
        "print(\"train X shape: \", X_train.shape) \n",
        "print(\"train y shape: \", y_train.shape) \n",
        "print(\"test X shape: \", X_test.shape)   \n",
        "print(\"test y shape: \", y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SR4gYffz_F_"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.transpose((3,0,1,2))\n",
        "X_test = X_test.transpose((3,0,1,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXYwWhHpz_GD"
      },
      "outputs": [],
      "source": [
        "print(\"train X shape: \", X_train.shape) \n",
        "print(\"train y shape: \", y_train.shape) \n",
        "print(\"test X shape: \", X_test.shape)   \n",
        "print(\"test y shape: \", y_test.shape)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auFZ63dtz_GH"
      },
      "outputs": [],
      "source": [
        "print(\"minimum train y value: \", min(y_train))\n",
        "print(\"maximum train y value: \", max(y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGHZvq4zz_GK"
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "    random_inx = np.random.choice(X_train.shape[0])\n",
        "    X_sample = X_train[random_inx, :, :]\n",
        "    plt.imshow(X_sample)\n",
        "    plt.show()\n",
        "    print(f\"Label: {y_train[random_inx]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4H2TOzTPel7"
      },
      "outputs": [],
      "source": [
        "# Normalize \n",
        "X_train = None\n",
        "X_test = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLbmJq5WPel7"
      },
      "outputs": [],
      "source": [
        "# change labels 10 to 0 (in both train and test sets)\n",
        "y_train[y_train==10] = 0\n",
        "y_test[y_test==10] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jMu79WCPel7"
      },
      "outputs": [],
      "source": [
        "print(\"minimum train y value: \", min(y_train))\n",
        "print(\"maximum train y value: \", max(y_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e7iSyWXz_GN"
      },
      "source": [
        "# 2. Multi-layer Perceptron (MLP) neural network classifier\n",
        "Construct a model to fit to the data. Using the Sequential API, build your model according to the following specifications:\n",
        "\n",
        "* Your model should use only Flatten and Dense layers, with the final layer having a 10-way softmax output.\n",
        "* The model should use the `input_shape` in the function argument to set the input size in the first layer.\n",
        "* The first layer should be a dense layer with 64 units.\n",
        "* The weights of the first layer should be initialised with the He uniform initializer.\n",
        "* The biases of the first layer should be all initially equal to one.\n",
        "* There should then be a dense layer, with 128 units.\n",
        "* Add batch normalization layer and dropuout layer (dropout rate = 0.2). \n",
        "* This should be followed with a dense layer, with 64 units.\n",
        "* Add batch normalization layer and dropuout layer (dropout rate = 0.2). \n",
        "* All of the Dense layers should use the ReLU activation function.\n",
        "* The output Dense layer should have 10 units and the softmax activation function.\n",
        "* Add weight decay (l2 kernel regularisation) in all Dense layers except the final softmax layer.\n",
        "* Print out the model summary (using the summary() method).\n",
        "* Compile and train the model (a maximum of 30 epochs), making use of both training and validation sets during the training run (use an Adam optimizer with learning rate = 0.001).\n",
        "* Your model should track at least one appropriate metric, and use at least one callbacks during training, such as a ModelCheckpoint callback.\n",
        "* As a guide, you should aim to achieve a final categorical cross entropy training loss of less than 1.0 (the validation loss might be higher).\n",
        "* Plot the learning curves for loss vs epoch and accuracy vs epoch for both training and validation sets.\n",
        "* Compute and display the loss and accuracy of the trained model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beEZO1kvz_GR"
      },
      "outputs": [],
      "source": [
        "model = Sequential([None])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxJXq3xYz_GU"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPPbzGhVz_GW"
      },
      "outputs": [],
      "source": [
        "model.compile(None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7b5_8VsCz_GZ"
      },
      "outputs": [],
      "source": [
        "# fit the model with validation set\n",
        "\n",
        "history = model.fit(None) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0kH6VYqz_Gc"
      },
      "outputs": [],
      "source": [
        "print(history.history.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjCtXGhPPel9"
      },
      "outputs": [],
      "source": [
        "# Plot the training and validation loss\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Loss vs. epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncPtDtCLz_Gg"
      },
      "source": [
        "## 3. CNN neural network classifier\n",
        "* Build a CNN classifier model using the Sequential API. Your model should use the Conv2D, MaxPool2D, BatchNormalization, Flatten, Dense and Dropout layers. The final layer should again have a 10-way softmax output. \n",
        "* the input layer with input_shape (32, 32, 3). \n",
        "* the first layer (conv1), has 6 filters with a shape of 5x5, with a relu activation function.\n",
        "* the second layer (pool1), is a max pooling layer, with size 2x2 and stride of 2x2.\n",
        "* the third layer (conv2), has 6 filters with a shape of 5x5, with a relu activation function.\n",
        "* the fourth layer (pool2), is a max pooling layer, with size 2x2 and stride of 2x2.\n",
        "* the fifth layer, flatten/unroll to a long vector.\n",
        "* the sixth layer, is a fully connected layer, with 120 units, with a relu activation function.\n",
        "* Add batch normalization layer and dropuout layer. \n",
        "* the seventh layer, is a fully connected layer, with 84 units, with a relu activation function.\n",
        "* Add batch normalization layer and dropuout layer. \n",
        "* the output (final) layer is a multi-class with 10 classes (activation function is softmax).\n",
        "* Print out the model summary (using the summary() method). Does the CNN model have fewer trainable parameters than your MLP model? why?\n",
        "* Compile and train the model (a maximum of 30 epochs), making use of both training and validation sets during the training run.\n",
        "* Your model should track at least one appropriate metric, and use at least one callbacks during training, such as a ModelCheckpoint callback.\n",
        "* Plot the learning curves for loss vs epoch and accuracy vs epoch for both training and validation sets.\n",
        "* Compute and display the loss and accuracy of the trained model on the test set.\n",
        "* Does your CNN model beat the MLP model performance?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yk2mH3Npz_Gh"
      },
      "outputs": [],
      "source": [
        "model = Sequential([None])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbgRgZ5cz_Gn"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkmS2vV2z_Gs"
      },
      "outputs": [],
      "source": [
        "model.compile(None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VytQECDVz_Gv"
      },
      "outputs": [],
      "source": [
        "# fit the model with validation set\n",
        "\n",
        "history = model.fit(None) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60mJypwQz_Gx"
      },
      "outputs": [],
      "source": [
        "print(history.history.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2v80qosz_G0"
      },
      "outputs": [],
      "source": [
        "# Plot the training and validation loss\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Loss vs. epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtqY1exHPel-"
      },
      "outputs": [],
      "source": [
        "# Choose a random test image\n",
        "\n",
        "random_inx = np.random.choice(X_test.shape[0])\n",
        "X_sample = X_test[random_inx, :]\n",
        "plt.imshow(X_sample)\n",
        "plt.show()\n",
        "print(f\"Label: {labels[y_test[random_inx]]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBxYRfN3Pel-"
      },
      "outputs": [],
      "source": [
        "# Get model predictions\n",
        "\n",
        "predictions = model.predict(None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVgii0ziPel_"
      },
      "outputs": [],
      "source": [
        "# Get the model prediction label\n",
        "print(np.argmax(predictions))\n",
        "print(f\"Model prediction:{labels[np.argmax(predictions)]}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}